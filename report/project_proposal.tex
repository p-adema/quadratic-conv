\title{}
\documentclass[11pt]{article} % Font size
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{a4paper, textwidth=450.0pt, textheight=740.0pt}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codeblue}{rgb}{0.16, 0.67, 0.72}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{backdark}{rgb}{0.12, 0.12, 0.13}
\definecolor{codeorange}{rgb}{0.81, 0.56, 0.43}
\lstdefinestyle{codestyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codeorange},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codegreen},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    columns=flexible,
}
\lstset{style=codestyle}
\title{	
    \vspace*{-1.5cm}
	\normalfont\normalsize
	\textsc{Bachelor KI}\\ Supervised by: Dr. ir. R. van den Boomgaard\\ % The course/subject name
	\vspace{3pt}
	\rule{\linewidth}{0.5pt}\\
	{\huge Project Proposal:\\Quadratic Forms in\\ Convolutional Neural Networks}\\ % The assignment title
	\vspace{4pt}
	\rule{\linewidth}{2pt}\\
	\vspace{4pt}
}
\author{
    \Large Peter Adema \\ 14460165
}
\date{\normalsize\today}  % Today's date (\today) or a custom date


\begin{document}
\maketitle % Print the title

\section{Literature Survey}
Modern Convolutional Neural Networks (CNNs) often use linear convolutional layers to process images and max-pooling layers to condense information and shrink the feature space \cite{introconvnets}. However, both of these operations are equivalent to a semifield convolution: the first in the linear field (with a learned kernel) and the second in the tropical-max field (with a step-function-like kernel) \cite{bellaardaxiomatic}. In \cite{bellaardaxiomatic}, Bellaard et al. provide an axiomatic foundation for using various semifields within the context of PDE-CNNs but do not discuss using semifields for conventional (discrete) CNNs. 

The ideas underlying the usage of tropical fields are, however, older than \cite{bellaardaxiomatic}: the field of mathematical morphology researches the shapes and forms of objects and functions, and two of the core operators within mathematical morphology are dilation (equivalent to a tropical-max correlation) and erosion (equivalent to a tropical-min correlation) \cite{maragos}. Heijmans \cite{heijmans1996morphological} is an excellent treatment of many of the theoretical fundaments and generalised cases of morphology, with Chapter 11 describing morphology for grey-scale images (most similar to the convolutional operations relevant to this project). Furthermore, morphological operations with specifically a quadratic structuring element were researched by Boomgaard in \cite{Boomgaard1999NumericalSS} and other papers, showing that many aspects of the resulting calculation can be performed in closed form without first approximating the quadratic into a fixed-size kernel.

Another paper of note regarding the efficient calculation of the convolutional stencil in tropical semifields may be \cite{fastanifilter}, in which Geusebroak and van de Weijer discuss how to perform an efficient calculation for the linear field with a Gaussian kernel. Whether such methods will be needed within this project is yet unclear, but the notes regarding approximate separability by reorienting the axes of a quadratic show a possible direction for further performance improvements.

Besides relevant theory, there are also some more recent pieces of literature somewhat close to this topic. Notably, \cite{qlin1, qlin2} show that a CNN that learns quadratic scale parameters for the kernels of its linear convolution can, in some cases, learn to perform tasks similar to those of a CNN that directly learns all kernel parameters. This adjustment significantly reduces the parameters required for the linear convolutions replaced in such a way. Furthermore, it is equivalent to the original Bachelor project proposal, where the task would have been to parameterise a linear convolution with the PDF of a Gaussian. 

Finally, previous projects under Dr. Boomgaard have partly investigated discrete subfield convolutions. The isotropic case (where scales are uncorrelated and bound between dimensions) for tropical-max fields has been relatively well-researched by \cite{thierrybsc, koenbsc}, showing minor performance increases in basic vision models. However, a more general treatment of anisotropic kernels in tropical max semifields (and other fields) is not yet present within either the public domain or the UvA collection of theses. 

\section{Research Question}
This project aims to rectify part of the abovementioned lack of a general treatment of discrete semifield convolutions for CNNs. By investigating whether using anisotropic kernels for tropical max-pooling can improve CNN performance. Additionally, this project will examine whether other variations on a semifield for max-pooling can have a similar effect and (time permitting) whether variations on the classical linear field for convolutions can increase model capabilities, the secondary goal of this project being to provide a starting point for future research surrounding the use of arbitrary semifields in CNNs, within the context of kernels parameterised by anisotropic quadratic forms. Finally, this project seeks to verify some theoretical claims regarding the behaviour of some semifields (such as the isometry of the log and tropical semifields) and some practical matters (such as the increasing of learned scales with larger images).

\section{Method and Approach}
In the first two weeks of the project, the components necessary for a tropical max and tropical min convolution were implemented and tested with some small models. Based on these initial results, it is clear that training an anisotropic model requires more care than an isotropic one, but the method is very feasible to implement. The remaining tasks, therefore, include:

\begin{itemize}
	\item Replicating the models from \cite{thierrybsc, koenbsc} more precisely for the isotropic baseline
	\item Attempting alternate initialisation schemes for anisotropic models
	\item Comparing the performance of step-function, isotropic and anisotropic kernel pooling layers in various CNN tasks.
	\item Investigating the effects of scaling up input images on learned scales
	\item Implementing non-selection-based semifields, such as the log semifields
	\item Investigating whether \verb torch.nn.functional.unfold  ~can also be implemented in terms of strides to reduce memory pressure
	\item (Time permitting) Investigate other properties of implemented semifields with respect to how models learn their parameters
	\item (Time permitting) Investigate the performance of non-linear convolutions replacing the linear convolutions in CNNs
	\item Read through parts of \cite{heijmans1996morphological} and \cite{bellaardaxiomatic} for relevant theory
And answering other questions that arise as part of the process.
\end{itemize}










\section{Evaluation}
The goal of this project is to characterise the performance and behaviour of various semifields when used in CNNs, and evaluation of this project can therefore be seen as consisting of two parts:

1) Ensuring the semifields are correctly implemented: this can be done by verifying that theoretical guarantees (such as characteristics of adjoint operators from morphology) hold, as well as comparing with other implementations (such as by [Thierry, Koen])
2) Using the created models on typical CNN tasks: by using conventional architectures and standard datasets, the comparison with traditional CNNs can be made clearer, and possible implementation errors can be more easily avoided.

\newpage
\section{Plan}
In the project's first week, a very simplistic version of a tropical convolution was implemented, and some small models were fit using it. Subsequently, the second week was spent improving the implementation of the tropical convolution and writing these exceptionally useful documents. For the upcoming weeks, an approximate plan for the programming and experiments could be the following:

\begin{enumerate}
	\item[Week 3] Replicate baseline models and attempt isotropic initialisation 
	\item[Week 4] Attempt radial initialisation, write and train models for more datasets
	\item[Week 5] Implement other semifields, finish tasks from the previous two weeks, start work on presentation
	\item[Week 6] Prepare and present midterm status update, investigate effects of scaling images
	\item[Weeks 7-9] Other experiments (time permitting), such as investigating swapping linear convolutions out, transition to writing the report. Possibly, work on Honours extension.
	\item[Week 10] Writing proper final versions of all experiments for replicability, rerunning all experiments for final results
	\item[Weeks 11-12] Fixing any potential errors in experiments
\end{enumerate}

Additionally, work on the background section of the report started in the first two weeks, and the required scope for the section became clearer as the project progressed. Writing the background section will parallel the experiments as feedback is provided (providing the supervisor's time). An approximate goal for the background section would be to have a mostly complete version in week 5, but this deadline may be moved a week forward or back, depending on the project's progress. After performing experiments that seem promising in terms of inclusion in the final report, a short piece with relevant figures will be written after the experiment concludes. For the main section of the report, an approximate plan could be the following:

\begin{enumerate}
	\item[Weeks 3-6] (Mostly) Finish Background section
	\item[Week 7] Outline of (almost) all experiments
	\item[Week 8] First version for all experiments
	\item[Week 9] First version of the introduction and the conclusion
	\item[Week 10] Improve first drafts of report sections based on feedback. Possibly, work on Honors extension.
	\item[Week 11] Finalise all sections of the report
	\item[Week 12] Final checks before handing in
\end{enumerate}

Within the above two plans, there is enough slack to compensate for unexpected occurrences or circumstances requiring an alternative approach. These plans (especially for the experiments) will likely be extended with new ideas as the project progresses.

\newpage
\bibliographystyle{ieeetr}
\bibliography{references}
\end{document}