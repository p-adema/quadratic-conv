\title{Literature Draft}
\documentclass[11pt]{article} % Font size
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{apacite}
\usepackage{caption}
\usepackage{geometry}
\geometry{a4paper, textwidth=400.0pt, textheight=740.0pt}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codeblue}{rgb}{0.16, 0.67, 0.72}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{backdark}{rgb}{0.12, 0.12, 0.13}
\definecolor{codeorange}{rgb}{0.81, 0.56, 0.43}
\lstdefinestyle{codestyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codeorange},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codegreen},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    columns=flexible,
}
\def\comment#1{\color{red}#1\color{black}}
\lstset{style=codestyle}

\usepackage{bbold}
\title{	
    \vspace*{-1.5cm}
	\normalfont\normalsize
	\textsc{Bachelor Thesis AI}\\ % The course/subject name
	\vspace{3pt}
	\rule{\linewidth}{0.5pt}\\
	\vspace{14pt}
	{\huge Literature Draft}\\ % The assignment title
	\vspace{4pt}
	\rule{\linewidth}{2pt}\\
	\vspace{4pt}
}
\author{
    \Large Peter Adema \\ 14460165
}
\date{\normalsize\today}  % Today's date (\today) or a custom date


\begin{document}
\maketitle % Print the title

\section{Problem statement}
Modern Convolutional Neural Networks (CNNs) often use linear convolutional layers to process images and max-pooling layers to condense information and shrink the feature space. However, it can be shown that both of these operations are equivalent to a semifield convolution: the first in the linear field (with a learned kernel) and the second in the tropical-max field (with a step-function-like kernel). More semifields have been examined in the domain of continuous (PDE) CNNs, but this does not seem to have been done yet for discrete CNNs. This project aims first to examine the use cases on anisotropic kernels for pooling and quantify any performance improvements. Afterwards, the use of other semifields in semifield convolutions will be examined within the context of kernels parameterised as quadratic forms ($x^TQ^{-1}x$, with Q positive semi-definite). The performance (accuracy and training/inference speed) of various CNN architectures modified to use semifield convolutions will be evaluated. Additionally, some theoretical assumptions regarding how the semifields should behave will be verified (e.g. scaling the image should increase the learned quadratic form scales, and log-semifields should approximate tropical min- and max-fields in the limits). 
\section{Keywords}
convolution, semifield, quadratic form, dilation, morphology, pooling, CNN

\newpage
\section{Some relevant literature}
\begin{enumerate} 
\item \citeA{bellaardaxiomatic}\\
	This paper provides a very useful overview of semifield theory as it pertains to PDE-CNNs, with a sizeable portion also being usable for this (discrete) case.
\item \citeA{fastanifilter} \\
	This paper shows how an anisotropic Gaussian filter can be efficiently calculated by using one axis-aligned convolution and one off-axis convolution, with relatively low approximation error. It would be interesting to apply something similar for this project, but I fear the reverse-mode AD would be very painful.
\item \citeA{henk2000fundamenta} and accompanying book \citeA{heijmans1996morphological}\\
	This paper (and book by same author) is a thorough mathematical description of morphological theory, and is good reference material.
\item \citeA{Boomgaard1999NumericalSS}\\
	This paper describes some relevant details for how a dilation can be performed with a quadratic kernel.
\item \citeA{smetspde}, and \citeA{groupequiv}\\
	  These two papers provide more background material on group-invariant operations in CNNs, both in the discrete and continuous case. They are mainly relevant for better understanding the project and \citeA{bellaardaxiomatic}.
\item \citeA{thierrybsc}\\
	My predecessor in this project has some useful notes regarding how the isotropic version of the quadratic pooling layer should behave, and the experiments will serve as a good starting point and baseline.
\item \citeA{ppad}\\
	This paper presents some useful concepts for efficient differentiation for various second order functions: in particular, the differentiation rule for \verb reduce  is relevant for computing the gradient in an arbitrary field correlation $\textcircled{$\star$}$.
\item \citeA{qlin1}  / \citeA{qlin2}\\
	These two papers propose using a quadratic function $x^TWx$ as the kernel in the standard convolutional layers of a CNN. This is very similar to the original project proposal, if we use the \textit{log}-PDF of the Gaussian as our kernel.
\end{enumerate}
\newpage
\bibliography{references}
\bibliographystyle{apacite}

\end{document}