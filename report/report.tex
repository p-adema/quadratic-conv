\documentclass[a4paper, 12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{xcolor}
\usepackage{afterpage}

\usepackage{relsize}
\usepackage{moresize}

\usepackage{graphicx}
\usepackage{geometry}

% [CHANGE] The title of your thesis. If your thesis has a subtitle, then this
% should appear right below the main title, in a smaller font.
\newcommand{\theTitle}{Quadratic Forms in \\
\vspace{0.5em}
Convolutional Neural Networks}
\newcommand{\theSubTitle}{}


% [CHANGE] Your full name. In case of multiple names, you can include their
% initials as well, e.g. "Robin G.J. van Achteren".
\newcommand{\theAuthor}{Peter Adema}

% [CHANGE] Your student ID, as this has been assigned to you by the UvA
% administration.
\newcommand{\theStudentID}{14460165}

% [CHANGE] The name of your supervisor(s). Include the titles of your supervisor(s),
% as well as the initials for *all* of his/her first names.
\newcommand{\theSupervisor}{Dr.\ ir.\ R.\ van den Boomgaard} % Dr. Ing. L. Dorst

% [CHANGE] The address of the institute at which your supervisor is working.
% Be sure to include (1) institute (is appropriate), (2) faculty (if
% appropriate), (3) organisation name, (4) organisation address (2 lines).
\newcommand{\theInstitute}{
Informatics Institute \\ %Institute for Logic, Language and Computation
Faculty of Science\\
University of Amsterdam\\
Science Park 900 \\ 
1098 XH Amsterdam 
}

% [CHANGE] The semester in which you started your thesis.
\newcommand{\theDate}{Semester 2, 2024-2025}



\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathabx}
%\usepackage{apacite}
\usepackage{caption}
\usepackage{listings}
\usepackage{xcolor}
\def\comment#1{\color{red}#1\color{black}}
\usepackage{bbold}
\DeclareMathOperator{\boxclose}{\vcenter{\hbox{$\blacksquare$}}}
\DeclareMathOperator{\boxopen}{\Box}


\begin{document}
\pagestyle{empty}
\begin{center}

\vspace{2.5cm}


\begin{Huge}
% see definition at beginning of document
\theTitle
\end{Huge} \\

\vspace{0.5 cm}

\begin{Large}
\theSubTitle
\end{Large}

\vspace{1.5cm}

% see definition at beginning of document
\theAuthor\\
% see definition at beginning of document
\theStudentID

\vspace{1.5cm}

% [DO NOT CHANGE]
Bachelor thesis\\
Credits: 18 EC

\vspace{0.5cm}

% [DO NOT CHANGE] The name of the educational programme.
Bachelor \textit{Kunstmatige Intelligentie} \\
\vspace{0.25cm}
\includegraphics[width=0.075\paperwidth]{figures/uva_logo} \\
\vspace{0.1cm}

% [DO NOT CHANGE] The address of the educational programme.
University of Amsterdam\\
Faculty of Science\\
Science Park 900\\
1098 XH Amsterdam

\vspace{2cm}

\emph{Supervisor}\\

% see definition at beginning of document
\theSupervisor

\vspace{0.25cm}

% see definition at beginning of document
\theInstitute

\vspace{1.0cm}

% see definition at beginning of document
\theDate

\end{center}
\newpage



\pagenumbering{arabic}
\setcounter{page}{1}
\pagestyle{plain} 

\section*{Abstract \comment{TODO}}


\section*{Acknowledgements \comment{TODO}}


\tableofcontents

\newgeometry{a4paper, textwidth=400.0pt, textheight=740.0pt}
\chapter{Introduction}
In the field of computer vision, machine learning has been successfully applied for societal benefit in various ways, ranging from analysing medical imaging data \cite{esteva2021deep, jain2015computer} to classifying agricultural produce \cite{wan2020faster, sivaranjani2022overview} and recognising license plate numbers \cite{xie2018new}. One of the primary tools used in these applications is the Convolutional Neural Network (CNN) \cite{le1990handwritten}, a type of neural network that leverages existing theoretical knowledge of image processing to learn representations of images more efficiently.

Two components are often seen within a typical CNN: the eponymous convolutional layer, which analyses the image, and a pooling layer, which compresses the image representation (see \cite{introconvnets} for an introduction). The latter pooling layer is often implemented as a max-pooling layer, which selects the highest value in a small area surrounding every point in the image. Slightly more generally, a max-pooling layer can be seen as an operation that weights neighbouring pixels around a point in the image and selects the pixel with the highest weight, but with all neighbours being weighted equally. However, this general perspective suggests the possibility of a variation on a max-pooling layer where pixels are not weighted equally: instead, pixels further away from the centre could be considered less in the selection for the maximum.

This idea has been formalised in mathematical morphology as dilation \cite{heijmans1996morphological} and in tropical geometry as a max-plus convolution \cite{maragos}. Using this formalism, a separate function $G$ (the structuring element or kernel) defines the weighting for neighbouring pixels. This function can be parameterised in various ways, but a concave function centred around the origin is typically used for G. One such function is the quadratic function $f(x)=x^T\Sigma^{-1}x$, and another is its isotropic form $f(x)=x^T(sI)^{-1}x$, with parameters $\Sigma$ and $s$ respectively \cite{Boomgaard1999NumericalSS}.

Previous studies \cite{groenendijk2022morphpool} and theses \cite{thierrybsc, koenbsc} have investigated the possibility of using such a dilation (generalised max-pooling) with an isotropic quadratic structuring element as a layer within a CNN, with the parameter $s$ being learned via gradient descent (a standard optimisation method within machine learning). This previous research showed that using an isotropic quadratic kernel (which is strictly more expressive than a standard max-pooling) resulted in higher performance on a small selection of datasets. This thesis aims to expand upon this by examining the possibility of using an anisotropic quadratic structuring element within the dilation, specifically whether the anisotropic parameters $\Sigma$ could also be learned via gradient descent. The expectation is that, since anisotropic quadratic kernels are again strictly more expressive than the isotropic versions, such a dilation layer would further improve performance.

\newpage
\section{Related work}
Modern Convolutional Neural Networks (CNNs) often use linear convolutional layers to process images and max-pooling layers to condense information and shrink the feature space \cite{introconvnets}. However, both of these operations are equivalent to a semifield convolution: the first in the linear field (with a learned kernel) and the second in the tropical-max field (with a step-function-like kernel) \cite{bellaardaxiomatic}. In \cite{bellaardaxiomatic}, Bellaard et al.\ provide an axiomatic foundation for using various semifields within the context of PDE- (continuous) CNNs but do not discuss using semifields for conventional (discrete) CNNs. 

However, the ideas underlying tropical fields are older than \cite{bellaardaxiomatic}. The field of mathematical morphology researches the shapes and forms of objects and functions, and two of the core operators within mathematical morphology are dilation (equivalent to a tropical-max / max-plus convolution) and erosion (close to a tropical-min correlation) \cite{maragos}. Heijmans provides an excellent treatment of many of the theoretical fundamentals and generalised cases of mathematical morphology in \cite{heijmans1996morphological}, with Chapter 11 describing morphology for grey-scale images (most similar to the convolutional operations relevant to this project). Furthermore, morphological operations with specifically a quadratic structuring element are well-studied, e.g.\ Boomgaard showing in \cite{Boomgaard1999NumericalSS} that many parts of the calculation can be done in closed form without first approximating the quadratic as a fixed-size kernel.

Another paper of note regarding the efficient calculation of the convolutional stencil in tropical semifields may be \cite{fastanifilter}, in which Geusebroek and van de Weijer discuss how to perform an efficient calculation for the linear field with a Gaussian kernel. However, due to time constraints, this method was not implemented for the anisotropic semifield convolutions in this report.

Besides relevant theory, there is also some more recent experimental research bordering on this topic. Notably, \cite{qlin1, qlin2} show that a CNN that learns quadratic scale parameters for the kernels of its linear convolution can sometimes learn to perform tasks similar to those of a CNN that directly learns all kernel parameters. This adjustment significantly reduces the number of  parameters required for the linear convolutions replaced in such a way. Furthermore, it is equivalent to the original Bachelor project proposal, where the task would have been to parameterise a linear convolution with the PDF of a Gaussian.
Also close to the topic, \cite{fan2021alternative} investigate (with unfortunately unclear results) replacing linear convolutional layers with max-plus convolutions, arguing that replacing multiplication with addition may result in models using less power.

Finally, \cite{groenendijk2022morphpool} and previous projects under Dr Boomgaard have partly investigated discrete semifield convolutions. The isotropic case (where scales are equal in all directions) for tropical-max fields has been relatively well-researched by \cite{thierrybsc, koenbsc}, showing minor performance increases in basic image classification tasks. However, a more general treatment of anisotropic kernels in tropical max semifields (and other semifields) is not yet present within the public domain or the UvA collection of theses.

\chapter{Background}
First, mathematical formalisms must be covered to understand the concepts discussed in later sections. These include the convolutional operator, its generalisation using semifields, and relevant semifields from mathematical morphology. Subsequently, we will discuss variations on the convolutional operator, as well as quadratic distance functions and a method for learning the positive definite matrices parameterising them.

\section{Convolutional operator}
At the core of a convolutional neural network is the convolutional operation $f*g$. The general form of a continuous convolution of functions $f$ and $g$ can be written as:
\begin{align}
(f*g)(x) &= \int_{y\in\mathcal{D}} f(x-y)\; g(y)	,
\end{align}
where $\mathcal{D}$ is the (continuous) domain of $f$ and $g$. However, save for a handful of functions whose convolutions can be calculated algebraically, we are typically required to approximate this convolution in the discrete domain. In this case, we approximate the functions $f$ and $g$ by sampling them at fixed intervals, the result of which can be represented as discrete arrays $F$ and $G$. A discrete convolution could then be written as \cite{szeliski2022computer}:
\begin{align}
(F*G)[x] &= \sum_{y\in\mathcal{Y}} F[x-y]\; G[y],
\end{align}
where $\mathcal{Y}$ is the set of all indices in the domain of $F$ and $G$ (e.g.\ $\mathcal{Y}=\mathbb{Z}^2$ for infinitely large 2D images and kernels), and $G$ is typically referred to as the (convolutional) kernel.


\section{Fields and semifield convolutions}
In the convolutional operator, a part of the image is repeatedly multiplied element-wise with a kernel, and the resulting values are summed to obtain an activation for each point. However, while we typically use scalar addition and multiplication in this calculation, it is also possible to use different operators in the reduction by defining a different field in which the reduction is done. In this section, we will briefly look at the concept of fields insofar as they are relevant to implementing an alternative version of the convolutional operator.

In mathematics, a field is a set of values with a pair of operators: one operator corresponds to the concept of addition, and one operator corresponds to the concept of multiplication. Fields are, in effect, a generalisation of standard addition and multiplication on integers or reals that allow for describing a set of values other than typical scalars or an alternate method for combining typical numbers. Formally, a field can be described as a tuple $(\mathcal{F}, \oplus, \otimes)$, where the operators $\oplus$ and $\otimes$ are of the type $\mathcal{F}\times\mathcal{F}\rightarrow\mathcal{F}$. Furthermore, the operators $\oplus$ and $\otimes$ are both beholden to the field axioms: informally, a set of rules to ensure they act 'similarly' to standard scalar addition and multiplication. 

These field axioms can be written as (adapted from \cite{beachy2006abstract} and \cite{bellaardaxiomatic}):
\begin{align}
\textrm{$\oplus$ is associative: }&\forall a,b,c\in \mathcal{F} &  a \oplus (b \oplus c) = (a\oplus b) \oplus c \\ 
\textrm{$\otimes$ is associative: }&\forall a,b,c\in \mathcal{F} &  a \otimes (b \otimes c) = (a\otimes b) \otimes c \\ 
\textrm{$\oplus$ is commutative: }&\forall a,b\in \mathcal{F} & a\oplus b = a  \oplus a \\
\textrm{$\otimes$ is commutative: }&\forall a,b\in \mathcal{F} & a\otimes b = a  \otimes a \\
\oplus\textrm{ has an identity: }& \exists \mathbb{0} ~\forall a\in \mathcal{F} & a\oplus \mathbb{0} = a \\ 
\otimes\textrm{ has an identity: }& \exists \mathbb{1} ~\forall a\in \mathcal{F} & a\otimes \mathbb{1} = a \\ 
\oplus\textrm{ has inverse elements: }& \forall a~\exists b\in \mathcal{F} & a\oplus b = \mathbb{0} \label{eq:additive-inverse}  \\ 
\otimes\textrm{ has inverse elements: }& \forall a~\exists b\in \mathcal{F} & a\otimes b = \mathbb{1}  \\ 
\mathbb{0} \textrm{ is absorbing for $\otimes$: }&\forall a\in \mathcal{F} & a\otimes \mathbb{0} = \mathbb{0}\\
\otimes \textrm{ distributes over $\oplus$: }&\forall a,b,c\in \mathcal{F} & a\otimes (b \oplus c) = (a\otimes b)\oplus(a\otimes c)
\end{align}

One use case for fields in machine learning is to describe a weighted reduction (as in a kernel-based convolution) more generally. To better understand this, let us return to the example of the convolutional operator. In this case, we must reduce the neighbourhood around a pixel $x$ in the image $F$, weighted by the values in the kernel $G$. While this would typically be written as:
\begin{align}
	(F*G)[x] &= \sum_{y\in\mathcal{Y}} F[x-y]\; G[y],
\end{align}

\noindent
we could instead use a field $S=(\mathcal{F}, \oplus, \otimes)$ and write a similar operation:

\begin{align}
	(F ~\textcircled{$*$}_S\; G)[x] &=\bigoplus_{y\in \mathcal{Y}} F[x-y] \otimes G[y] \label{eq:semifield-conv}
\end{align}
\noindent
Performing this operation does not, strictly speaking, require any of the above field axioms to hold for $\oplus$ or $\otimes$: the only relevant restriction would be that of the types being $\mathcal{F}\times\mathcal{F}\rightarrow\mathcal{F}$. Implementing this operation for a CNN would further require both operators to be differentiable, and for $\mathbb{0}$ to exist (to deal with border effects in a convolution). However, it is generally also useful for the reduction operator $\oplus$ to be both associative and commutative, as this makes the order of elements in the input irrelevant, promoting the stability of the result and allowing efficient parallel implementation of the reduction \cite{ppad}. Furthermore, most of the field axioms hold regardless for most operators we wish to use for $\oplus$ and $\otimes$. As such, we will use a semifield \cite{bellaardaxiomatic}:\\
\textbf{
A semifield is a field, except an additive ($\oplus$) inverse (Eq. \ref{eq:additive-inverse}) does not necessarily exist.
}

We can then define the operator $\textcircled{$*$}_S$ for the following sections (identically to Eq. \ref{eq:semifield-conv}) as the semifield convolutional operator for any semifield $L$.

\section{Semifields from mathematical morphology}
Knowing that an operator similar to convolution can be used in any semifield, we can examine if there are relevant semifields in which we can perform a convolution other than the standard linear field. For this, we can take inspiration from mathematical morphology, the study of object and function shapes.

Two core classes of discrete operators from mathematical morphology are dilations and erosions, where dilations informally correspond with 'making a function larger' (scaling the umbra of a function), and erosions with 'making a function smaller'. The result of the common dilation is shown in Fig. \ref{fig:dil-illust}. Examining the local effects of the dilation more closely, we can see that it is somewhat similar to taking a local maximum, which can also be seen in the formula for this dilation operator $\boxplus$  (from \cite{heijmans1996morphological}, using the Minkowski sum): 
\begin{align}
F \boxplus G \textrm{, where }(F \boxplus G)[x] = \bigvee_ {y\in\mathcal{Y}} \left(F[x-y] + G[y]\right)	
\end{align}
\noindent
Here, $F$ is the image (or object or sampled function) to be dilated, $G$ is a structuring element describing how the dilation will occur, and $\bigvee$ denotes the supremum. If we further restrict $F$ and $G$ to be of finite size, then $\mathcal{Y}$ will be of finite size, and the correspondence with the local maximum becomes exact:
\begin{align}
\textrm{For finite-size }F\textrm{ and }G: (F \boxplus G)[x] = \max_ {y\in\mathcal{Y}} \left(F[x-y] + G[y]\right)	
\end{align}
 An intuitive explanation would be to see the structuring element $G$ as a (negated) distance function and the dilation $\boxplus$ as the operation that takes the highest value weighted by how 'close' it is to $x$. If $G$ is a step function with value zero near its centre and $-\infty$ outside (Fig. \ref{fig:dil-illust}, first row), we can see that this is precisely taking the maximum value in the area where $G$ is zero. However, we may also wish to use a quadratic (Fig. \ref{fig:dil-illust}, second row) or other function as $G$. Depending on $G$, we may still be able to perform the dilation exactly (using algebraic solutions and/or leveraging separability), but to compute the dilation in the general case, we may wish to clip $G$ to be above $-\infty$ in only a constrained domain (Fig. \ref{fig:dil-illust}, third row). The dilation with the clipped version of $F\boxplus G_{clipped}$ could then be seen as an approximation of the dilation $F\boxplus G$ with the full (unclipped) $G$ while having the advantage that the set of relevant indices $\mathcal{Y}$ is bounded in size. It should be noted, however, that if $F$ is $K$-Lipschitz and $G$ is strictly decreasing with the second derivative $|\delta^2 G|>K$ outside a central area, then clipping $G$ to this central area will not change the result of the dilation ($F\boxplus G_{clipped}$ is then exact, see Appendix \ref{sec:lipschitz}).

Looking at the operation performed by dilation more closely, we can see that it is, in effect, a maximum operation weighted by a distance function. Similarities with the weighted reduction in the semifield convolution $\textcircled{$*$}_S$ may lead us to believe that this can also be viewed as a convolution in the appropriate semifield $S$, and this is indeed the case. By defining $\oplus=\max$ and $\otimes=+$, we obtain the tropical max semifield $T_+=(\mathbb{R}\cup\{-\infty\},\max,+)$ with neutral elements $(\mathbb{0}=-\infty, \mathbb{1}=0)$ \cite{maragos, bellaardaxiomatic}. A convolution $F \;\textcircled{$*$}_{T_+} G$ in this tropical semifield $T_+$ (also known as a max-plus convolution \cite{maragos}) would then be $\boxplus$:
\begin{align*}
	(F ~\textcircled{$*$}_{T_+} G)[x]
	&= \bigoplus_{y\in \mathcal{Y}}F[x-y] \otimes G[y] \\
	&= \max_{y\in \mathcal{Y}}\left(F[x-y] + G[y]\right) \\
	&= (F \boxplus G)[x]
\end{align*}

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{figures/dilation_illustration.png}
  \caption{Illustration of the effects of the dilation $\boxplus$ with three kernels on a sinusoidal $f$. $g_{step}=0$ in a region of size 0.5 and $-\infty$ outside, while $g_{quad}=-5x^2$ and $g_{qclip}=g_{quad}+g_{step}$. An alternative intuition for $\boxplus$ is also illustrated, corresponding with 'lowering' a negated version of $g'$ down towards the point $x$ until it intersects $f$ and taking the value of the lowered and flipped $g'(x)$ as the result of $\boxplus$ at point $x$. Note the continuous, thus lowercase $f$ and $g$.}
  \label{fig:dil-illust}
\end{figure}
\noindent
This result is interesting because we can see the standard max pooling layer in a convolutional neural network as a dilation with a fixed, step-function-like $G$ (a 2D version of $G_{step}$ from Fig. \ref{fig:dil-illust}). Generalising $G$ to be a quadratic form is a logical next step, where this thesis focuses on the anisotropic case.

In mathematical morphology, dilations and erosions come in pairs named adjunctions. It can be shown that the erosion which adjoins $\boxplus$ (henceforth referred to as the operator $\boxminus$) is effectively a minimum (infimum in the infinite case) weighted by a (negated) distance function \cite{heijmans1996morphological}: 
\begin{align}
	(F \boxminus G)[x]=\min_{y\in \mathcal{Y}}(F[x+y] - G[y])
\end{align}
\noindent
Using similar logic as above, we can show that, in the corresponding tropical min semifield $T_-=(\mathbb{R}\cup\{\infty\},\min,+)$\footnote{For both $T_+$ and $T_-$, it should be noted that standard addition $+$ is undefined for $\pm\infty$. In accordance with \cite{maragos}, we define $\forall x: x+(-\infty)=(-\infty)$ in $T_+$, while $\forall x: x+\infty=\infty$ in $T_-$} \cite{maragos} with neutral elements $(\mathbb{0}=\infty, \mathbb{1}=0)$, the convolution with $\check{G}^*(x)=-G(-x)$ is equivalent to the erosion $\boxminus$:
\begin{align}
	(F ~\textcircled{$*$}~ \check{G}^*(x))[x]
	&= \bigoplus_{y\in \mathcal{Y}}F[x-y] \otimes \check{G}^*(x)[y] \\
	&= \min{y\in \mathcal{Y}}(F[x-y] + \check{G}^*(x)[y]) \\
	&= \min{y\in \mathcal{Y}}(F[x-y] - G[-y]) \\
	&= \min{y^*\in \mathcal{Y}}(F[x+y^*] - G[y^*]) \\
	&= (F \boxminus G)[x]
\end{align}

\section{Further relevant mathematical morphology}
Besides the common adjoint $(\boxplus,\; \boxminus)$ using $+$ and $-$ as the operators weighting each point, one can also define an adjoint that uses multiplication and division. This alternate adjoint on $\mathbb{R}_+$ then has the dilation $\dot \boxplus$, defined as \cite{heijmans1996morphological}:
\begin{align}
	(F~ \dot\boxplus~ G)[x] &= \bigvee_{y\in \mathcal{Y}}F[x-y]G[y]
\end{align}
However, it can be shown that this adjoint is equivalent (anamorphic) to the common adjoint $(\boxplus,\; \boxminus)$ using the transformation $t(x)=e^x$ and its inverse $t^{-1}(x)=\log(x)$. As such, using $\dot \boxplus$ would not result in a more expressive convolution while complicating implementation with the requirement of $\mathbb{R}_+$.

Two other concepts from mathematical morphology that do increase expressivity are the concepts of openings and closings \cite{heijmans1996morphological, gonzalez2017}. Here, an opening is an operator using an adjoint, where the input is first eroded and then dilated with the same structuring element $G$. Notably, an opening is anti-extensive (decreasing), meaning that for $(\boxplus,\; \boxminus)$ we can write the opening $\boxopen$:
\begin{align}
	\forall x\in\mathcal{Y}~~(F ~\boxopen~ G)[x] &= ((F\;\boxminus\; G)\;\boxplus\;G)[x] \leq F[x]
\intertext{
The operator dual to an opening is the closing using the same adjoint: here, a closing refers to first performing a dilation, and then performing the adjoining erosion with the same structuring element $G$. Notably, a closing is extensive (increasing), meaning that for $(\boxplus,\; \boxminus)$ we can write the closing $\boxclose$:
}
	\forall x\in\mathcal{Y}~~(F ~\boxclose~ G)[x] &= ((F\;\boxplus\; G)\;\boxminus\;G)[x] \geq F[x]
\end{align}
One of the possible advantages of using closing and opening over only dilation or erosion is that opening and closing both produce results with contours similar to their inputs \cite{gonzalez2017}. Illustrating the effects of opening and closing with a simple diagonal kernel in Figure \ref{fig:closing-illust}, we can see that the results of the closing have a higher value (due to the extensivity of $\boxclose$), but the shapes are less distorted by the shape of the kernel $G$ when compared with the results of the dilation. 

This property, when combined with the extensivity of $\boxclose$, suggests it can also be used in the place of a dilation within the context of max-pooling for CNNs. As such, we will later apply the closing $\boxclose$ in that context, implemented as not a single semifield convolution \textcircled{$*$}, but a pair in $T_+$ and $T_-$ respectively.

%In conclusion, we can find two candidates for inclusion in CNNs within the field of mathematical morphology: firstly, the dilation $\boxplus$, and secondly, the closing $\boxclose$, both to be used as a replacement for a max-pooling layer and implemented as (a pair of) semifield convolutions \textcircled{$*$} within tropical semifields.


\begin{figure}[hb!]
	\center
  \includegraphics[width=\textwidth]{figures/closing.png}
  \caption{Illustration of the effects of opening and closing, where warmer (red) colors  indicate higher values, and colder (blue) colors  indicate lower values.}
  \label{fig:closing-illust}
\end{figure}

\section{Variations on the convolution in CNNs}
Within a Convolutional Neural Network, there are layers typically referred to as convolutional layers. These layers apply an operation very similar to a discrete mathematical convolution $*$ of the input activations (or image) and a parameterised kernel (here with an explicit domain $\mathcal{X}$ for valid values of $x$):
\begin{align}
\forall x\in\mathcal{X}:~~(F*G)[x] &= \sum_{y\in\mathcal{Y}} F[x-y]\; G[y]
\end{align}
However, a CNN convolution has additional parameters that may change its behaviour. To better understand how we can apply semifield convolutions $\textcircled{$*$}$ in CNNs, we must therefore understand these parameters, usually named 'stride', 'dilation', 'padding'  and 'groups' in modern deep learning frameworks \cite{noauthor_conv2d_nodate, noauthor_xla_nodate}. Additionally, images and activations in CNNs have 'channels': an additional axis in the input, which is treated differently from the spatial ones. We will first discuss stride, dilation and padding, which do not interact with channels, and then discuss the concept of channels and convolutional groups.

Stride controls the spacing of the sampling grid for $F$ with respect to the output position $x$: where a regular convolution uses $F[x-y]$, a strided convolution would have $F[\textsc{stride}\times x - y]$. Striding can also be seen as 'moving' the receptive field (accessed values of $F$) of the convolution with a step size equal to the stride: see Fig. \ref{fig:stride-dilation-padding}. For finite images, this reduces the size of the convolution result by shrinking the set of valid indices $\mathcal{X}$. Therefore, a $\textsc{stride}>1$ can allow later convolutions to be influenced by more inputs without increasing kernel sizes. A standard convolutional layer typically has a stride of 1, but a pooling layer might have a stride of 2 or higher.

Dilation is similar, but instead adjusts the step size for $y$: the term $F[x-y]$ becomes $F[x-\textsc{dilation}\times y]$, creating 'gaps' between the sampling points for $F$ (e.g.\ for $x=1$, sampling $F[-1]$, $F[1]$, $F[3]$) without changing the sampling for the kernel $G$. However, a $\textsc{dilation}>1$ results in a locally non-smooth operator, (which seems undesirable for smooth pooling). As such, none of the later experiments investigated using a $\textsc{dilation}>1$.

\begin{figure}[h!]
	\center
  \includegraphics[width=0.87\textwidth]{figures/stride-dilation-padding.png}
  \caption{Illustration of the effects of stride, dilation and padding with a $3\times3$ kernel on a $6\times6$ input. The top row shows the receptive field of the reduction (accessed values of $F$), while the bottom row shows the output structure.}
  \label{fig:stride-dilation-padding}
\end{figure}

\noindent
Padding refers to convolving with a widened $F$: if a certain $F$ has domain $[0, 5]$, then a padding of 1 would correspond with convolving with an expanded $F'$ with domain $[-1, 6]$ that is equal to $F$ within $[0, 5]$. While there are many so-called border strategies \cite{gonzalez2017} in computer vision for how to determine the value of the newly added $F'[-1]$ and $F'[6]$, the method typically used within CNNs is to set these new values to 0, thereby keeping the sum unchanged. 
Our use of padding lies in ensuring that the size of the output image does not shrink with kernel sizes larger than 1: this simplifies network structures by ensuring the output size decreases only if $\textsc{stride}>1$. For an odd-valued kernel dimensionality $K_o$ (such as $3\times3$, see Fig. \ref{fig:stride-dilation-padding}), this can be achieved by setting the padding to $\lfloor\frac{K_o}{2}\rfloor$. Even-valued kernel dimensionalities $K_e$ depend on where we define the centre of the kernel to lie. Suppose we define it as towards the lower indices in the kernel (up and to the left in 2D images) and allow for different amounts of padding at the beginning and end of $F$: in that case, we can retain image sizes by padding with $\frac{K_e}{2}-1$ at the low-index side of $F$ (top/left) and padding with $\frac{K_e}{2}$ at the high-index side of $F$ (bottom/right).


Besides modifying the receptive field of the convolution, another way in which CNN convolutions can diverge from mathematical ones is in their usage of a channel axis, distinct from the spatial axes. A small 2D RGB image might be stored as a $3\times50\times50$ array, but while the output index $x$ in a CNN convolution would represent a position in the X- and Y- axes of the image, the channel (colour) axis is not indexed. Instead, every convolutional kernel is responsible for one channel in the output and reads from a fixed set of channels in the input: while the kernel is 'moved' through the spatial dimensions during the convolution, it stays fixed relative to the channel dimensions. In a standard CNN convolution, all kernels read from all input channels, meaning that a square convolution of size 5 in this RGB image would require a convolutional kernel of size $3\times5\times5$ (as the kernel reads from 3 channels). In contrast, a typical max-pooling works per channel: every convolution operation only reads from one input channel. However, both cases can be seen as an adjusted version of the standard convolution formula. For a linear convolution, we simply add a summation over the set of input channels $\mathcal{C}$, indexing both $F$ and $G$ \cite{noauthor_conv2d_nodate}:

\begin{align}
\forall x\in\mathcal{X}:~~(F*G)[x] &= \sum_{c\in\mathcal{C}} \sum_{y\in\mathcal{Y}} F_c[x-y]\; G_c[y]
\end{align}
\noindent
Groups are then the parameter which allows us to precisely characterise this set of input channels $\mathcal{C}$. If the number of input- and output channels is a multiple of a certain number $N_{groups}$, then we can split both input and output into $N_{groups}$ equally sized groups. Here, $N_{groups}$ is the parameter named 'groups' in deep learning frameworks. A kernel in the $i$'th group of the output then reads from all the input channels in the $i$'th group of the input. For example, suppose we have an image with 6 input channels ($C_i=6$): $N_{groups}=1$ would result in every kernel reading from every input channel (a standard convolution), while $N_{groups}=C_i=6$ would make kernels read from only one channel (like a pooling). Values of $N_{groups}$ where $1<N_{groups}<C_i$ interpolate between these extremes, where every kernel reads a fixed subset of the input channels. It should be noted that while the number of output channels $C_o$ (equal to the number of kernels) is constrained to be a multiple of $N_{groups}$, the number of kernels per group in the output $S_o$ can be freely chosen and does not need to equal the number of channels per input group $S_i$: see Fig. \ref{fig:channels-groups} for examples.

\begin{figure}[h!]
	\center
  \includegraphics[width=\textwidth]{figures/channels-groups.png}
  \caption{Illustration of channels in the input and output, and the effects of the group parameter on which input channels are used by the kernels.}
  \label{fig:channels-groups}
\end{figure}

\section{Non-morphological semifields}
In previous sections, we discussed two semifields related to mathematical morphology, namely $T_+$ and $T_-$, and showed their correspondence with the pooling layers within a CNN. Using these results, we could see that a max-pooling was equivalent to a semifield convolution (in $T_+$, with a flat kernel). However, the standard convolutional layers within a CNN can also be interpreted as a semifield convolution in the linear semifield
$L=(\mathbb{R}, +, \times)$: filling in the equation for $\textcircled{$*$}_L$ (Eq. \ref{eq:semifield-conv}), we would again obtain a standard convolution $*$.

This raises the question of whether there are alternative semifields that act similarly to the linear semifield, such that they could be used in place of the standard convolutional layer in a CNN. In an investigation of some semifields, \cite{bellaardaxiomatic} identified sets of relevant semifields that were isomorphic (equivalent under a bijective mapping) to the linear semifield $L$. These can be written as:

\begin{itemize}
	\item[$L_{\mu+}$]  The positive log semifields:  $(\mathbb{R}\cup \{-\infty\}, \oplus_\mu, +)$ for all $\mu>0$ where \\$a\oplus_\mu b= \frac{1}{\mu}\ln(e^{\mu a}+e^{\mu b})$, $\mathbb{0}=-\infty$ and $\mathbb{1}=0$ 
	\\(with $\forall x\;e^{x+(-\infty)}=e^{-\infty}=0$)
	\item[$L_{\mu-}$]  The negative log semifields:  $(\mathbb{R}\cup \{+\infty\}, \oplus_\mu, +)$ for all $\mu<0$ where \\$a\oplus_\mu b= \frac{1}{\mu}\ln(e^{\mu a}+e^{\mu b})$, $\mathbb{0}=+\infty$ and $\mathbb{1}=0$ 
	\item[$R_p$]  The root semifields:  $(\mathbb{R}_+, \oplus_p, \times)$ for all $p\ne0$ where \\$a\oplus_p b= \sqrt[p]{a^p+b^p}$, $\mathbb{0}=0$ and $\mathbb{1}=1$
\end{itemize}

\noindent
In \cite{bellaardaxiomatic} it was also noted that the positive log semifield $L_{\mu+}$ becomes equivalent to $T_+$ when $\mu$ approaches $+\infty$, while $L_{\mu-}$ becomes equivalent to $T_-$ as $\mu$ approaches $-\infty$. 
Additionally, one can see that the root semifields $R_p$ also become equivalent to $T_+$ when $p$ approaches $+\infty$, as $\oplus_p$ then becomes the infinity-norm (which is equivalent to $\max$). Informally, the reader may also convince themselves that as $p$ approaches $-\infty$, the semifield addition $\oplus_p$ approximates $\min$, making a theoretical $R_{-\infty}$ equivalent to $T_-$ (out of scope for this report).

These additional equivalences with $T_+$ suggest the possibility of using log or root semifields with high values for $\mu$ or $p$ as another alternative for traditional max-pooling layers in CNNs. It should be noted, however, that neither log nor root semifields seem to be part of an adjoint, meaning the resulting pooling would likely not be easily modelled using principles of mathematical morphology, and many of the theoretical guarantees would be lost.

% $\sqrt[\infty]{\frac{1}{\frac{1}{a^\infty}+\frac{1}{b^\infty}}}$

\chapter{Method}
With a greater understanding of semifields, convolutions, and some relevant examples of semifield convolutions, we can now examine how to apply semifield convolutions within the context of CNNs. 
First, we will go through some implementation notes on semifield convolutions and describe how to use quadratic forms to parameterise convolutional kernels. Afterwards, we will describe the experimental setup used to examine which quadratic forms work best in the context of CNNs and which parameters of a semifield convolution positively impact a CNN's performance on various image classification tasks.



\section{Semifield convolutions in a CNN}

Previous sections described the theory underlying a semifield convolution and the additional parameters available in a CNN, but applying this in practice requires an efficient implementation. For this purpose, the author wrote two packages: \verb pytorch-semifield-conv , which uses Just-In-Time (JIT) compilation to create convolution operators, and \verb pytorch-numba-extension-jit , which automatically generates C++ bindings for the operators and aids in the JIT compilation process. More details on these packages can be found in the Extension report and the accompanying package documentation.

While implementation details will be left to the Extension report, two notable differences exist between the semifield convolutions used in this report compared with the descriptions in previous works \cite{thierrybsc, koenbsc}. 

Firstly, the dilations used for this report do not spread the gradient between multiple maxima (i.e. if there are two equal maxima, assign both a gradient of 0.5 instead of picking one to have a 1.0 gradient). This spreading is the behaviour of the \verb torch.amax  function, but \verb torch.max  is more efficient and does not spread the gradient. Similarly, the CUDA kernels created for this report also do not spread the gradient. Some small-scale experiments were done to investigate whether spreading the gradient has a meaningful impact, but these showed no difference (as equal maxima are exceedingly rare).

Secondly, summation across channels is always done using semifield addition. For dilations, some have proposed using scalar addition instead of the maximum across channels \cite{koenbsc, fan2021alternative}. However, various small-scale experiments consistently showed slightly worse performance for quadratic kernels when replacing $\max$ with $+$ across channels. Since this report focuses solely on quadratic kernels, this modification was not considered during the experiments.



\newpage
\section{Generating dilation kernels}
For a standard linear convolution, we typically use a fully learned kernel. Since the reduction operation in the linear semifield is $+$, the partial derivative of the summation result is equal to a constant $1$ for all terms, and all parts of the kernel can typically receive a portion of the gradient during back-propagation.

However, this is not the case for dilations; a convolution in $T_+$ uses $\max$ as the reduction operator, meaning that only one term receives a non-zero gradient. If the kernel were fully parameterised (every value learned separately), fitting the kernel would be very challenging due to the overly sparse gradient. As such, we can instead choose to generate a kernel based on a parameterised function, reducing the number of parameters to be learned by gradient descent.

There are many options for generating a kernel-like array, but the context of a replacing a max-pool with dilation can help suggest reasonable constraints. Firstly, the result of a max-pool can never be below the original value; in a dilation, we can ensure this by setting the centre of the kernel to $0$. Secondly, the result of a max-pool is at most the maximum value in an area, and never higher; we can ensure this by using a nonpositive kernel: $\forall y,~G[y] \leq 0$. Finally, we may consider it reasonable for the kernel to be concave, as this ensures spatial locality (we cannot 'skip over' a pixel when looking for the maximum).


Combining these requirements, we can see that an alternate formulation for such a kernel function would be as a function that evaluates the distance to the centre of the kernel for all points, and uses the negative of that distance for the value of the kernel $G$. Formally, such a kernel function can be viewed as a metric (see \cite{metrics}) $d$ on the space of kernel indices $\mathcal{I}$, with the values at any point $\mathbf{y}\in\mathcal{I}$ in the kernel being the negated distance to the kernel centre $c_k$:
\begin{align}
	\forall \mathbf y,~~G[y]=-d(\mathbf y, c_k)
\end{align}
Using this formalism, we can see that appropriate kernel functions are all derived from metrics calculating a 2D distance. If we further set $c_k$ to be the origin of $\mathcal{I}$, we can write any distance function as $d(\mathbf y)$ (with the centre implicit, effectively equivalent to a vector norm). A straightforward distance we could then choose might be the squared distance $d(\mathbf y)=\mathbf y^T\mathbf y$, but this is nonparametric. Adding a parameter could then lead to the isotropic quadratic:
\begin{align}
	\textrm{Isotropic quadratic kernel: }&G[\mathbf y] = -\mathbf y^T(sI)^{-1}\mathbf y
\intertext{
Here, the scalar $s$ can be adjusted to control how quickly the distance rises and the kernel values fall. However, the isotropic quadratic only generates kernels where the contour lines are circles. To allow for non-circular kernels, we must allow for dimensions to be weighted differently. A candidate function could then be the anisotropic quadratic (the Mahalanobis distance to the origin):
}
	\textrm{Anisotropic quadratic kernel: }&G[\mathbf y] = -\mathbf y^T\Sigma^{-1}\mathbf y
\end{align}
By learning a $2\times2$ positive definite matrix for $\Sigma$, we can then parameterise a kernel function for a 2D dilation kernel $G$ with elliptical contour lines. Testing whether moving from circular contours to elliptical contours improves CNN performance is then the primary subject of this report and later experiments.

It should finally be noted that the choice for these quadratic kernels is not entirely arbitrary: these 'quadratic forms' have theoretical advantages in terms of the smoothness and well-behaviour when used for dilations, similar to how kernels based on their exponent (the Gaussian) act in linear convolutions.

\newpage
\section{Learning positive definite matrices}
\label{sec:learning-pdm}
In order to apply the aforementioned quadratic kernels in a neural network, we must first devise a parameterisation scheme that can be used for gradient-based optimisation. For both types of quadratic kernels, the equivalent of the Mahanalobis distance covariance matrix ($sI$ for isotropic, $\Sigma$ for anisotropic) is required to be positive definite. For an isotropic kernel, using a parameter $\theta$ where $\theta = \log s$ ensures that $s$ is positive, and thus that $sI$ is positive definite. However, the matrix $\Sigma$ for the anisotropic case requires more care: if the entire matrix were freely learned via gradient descent, it is entirely possible that an update may result in $\Sigma$ no longer being positive definite.

One possibility would be to view $\Sigma$ as a $2\times2$ covariance matrix, and parameterise it accordingly. Since $\Sigma$ must in this case be symmetric, we know by the spectral theorem that $\Sigma$ is diagonalisable \cite{poole2015linear}:
\begin{align}
\textrm{For some orthogonal matrix }Q &\in \mathbb{R}^{2\times 2} \textrm{, and} \nonumber \\
\textrm{for some diagonal matrix }D &\in \mathbb{R}^{2\times 2} \textrm{,}\nonumber \\
\Sigma &= QDQ^T\\
&= Q \begin{bmatrix}
	 \sigma_1^2 & 0\\ 0 &  \sigma_2^2
\end{bmatrix} Q^T
\end{align}
Here, Q (as an orthogonal matrix) can either be a rotation  or a reflection. However, since $Q$ occurs twice, its determinant cancels, and fixing $Q$ to be a rotation does not reduce expressivity (see Appendix \ref{sec:red-mirr}). As such, we can use:
\begin{align}
\Sigma = \begin{bmatrix}
	\cos \phi & -\sin \phi \\ \sin\phi & \cos\phi
\end{bmatrix}\begin{bmatrix}
	 \sigma_1^2 & 0\\ 0 &  \sigma_2^2
\end{bmatrix} \begin{bmatrix}
	\cos \phi & \sin \phi \\ -\sin\phi & \cos\phi
\end{bmatrix}
\end{align}
This parameterisation can be efficiently inversed for the quadratic form, as \begin{align}
	\Sigma^{-1} &= (QDQ^T)^{-1}\\
	&= (Q^T)^{-1} D^{-1} Q^{-1}\\
	&= Q\begin{bmatrix}
	 \frac{1}{\sigma_1^2} & 0\\ 0 &  \frac{1}{\sigma_2^2}
\end{bmatrix}Q^T
\end{align}
In order for $\Sigma$ to be positive-definite, $\sigma_1^2$ and $\sigma_2^2$ are required to be strictly positive, while there are no constraints on $\phi$. As such, for any $\boldsymbol{\theta}\in\mathbb{R}^3$:
\begin{align}
\textrm{Let }\boldsymbol{\theta}&=\begin{bmatrix}
	\theta_1\\
	\theta_2\\
	\theta_3\\
\end{bmatrix}=\begin{bmatrix}
	\log|\sigma_1| \\
	\log|\sigma_2|\\
	\phi\\
\end{bmatrix}, \textrm{ then a valid $\Sigma^{-1}$ would be} \\
\Sigma^{-1} &= \begin{bmatrix}
	\cos \theta_3 & -\sin \theta_3 \\ \sin\theta_3 & \cos\theta_3
\end{bmatrix}\begin{bmatrix}
	 e^{-2\theta_1} & 0\\ 0 &  e^{-2\theta_2}
\end{bmatrix} \begin{bmatrix}
	\cos \theta_3 & \sin \theta_3 \\ -\sin\theta_3 & \cos\theta_3
\end{bmatrix}
\end{align}
Since we placed no assumptions on $\boldsymbol{\theta}$, it is safe for a black-box or gradient-based optimiser to adjust in any direction, as the resulting $\Sigma$ will always be a positive definite matrix.

This parameterisation has the advantage of being easily interpretable: $e^{\theta_1}$ and $e^{\theta_2}$ are the standard deviations of a hypothetical normal distribution in the first and second principal axis of the quadratic, while $\theta_3$ is the counter-clockwise angle the first principal axis forms with the x-axis. An alternative parameterisation for a covariance matrix $\Sigma$, based on the Pearson correlation coefficient, can be found in Appendix \ref{sec:pearson}.




\newpage
\section{Parameter initialisation \comment{TODO}}
\comment{uniform, ss, ss-iso, spin}

\newpage
\section{Experimental setup \comment{TODO}}
\comment{Hardware, software, hyperparameter tuning}


\newpage
\section{Image classification datasets \comment{TODO}}
\comment{Which ones did I use?}


\chapter{Experiments \comment{TODO}}
\comment{I only really want to write the accompanying text when I have a clear idea of what the experiments are going to be: for now, I'm just putting some graphs in from the test experiments I conducted.}
\begin{figure}[h!]
	\center
  \includegraphics[width=\textwidth]{figures/morphological_3d.png}
  \caption{Illustration of the effects of dilation, erosion and their combination in two dimensions, with height representing the function value.}
  \label{fig:res1}
\end{figure}
\begin{figure}[h!]
	\center
  \includegraphics[width=\textwidth]{figures/thierry_replication.png}
  \caption{Replication of Blankenstein's LeNet results \cite{thierrybsc} }
  \label{fig:res2}
\end{figure}
\begin{figure}[h!]
	\center
  \includegraphics[width=\textwidth]{figures/100ep_extension.png}
  \caption{Extension of LeNet results using more (100) epochs, with the aniso results other than the base aniso-3/5/7 being obtained with higher LR}
  \label{fig:res3}
\end{figure}


\chapter{Conclusions \comment{TODO}}

\section{Findings \comment{TODO}}
\section{Discussion \comment{TODO}}
\section{Contributions \comment{TODO}}
\section{Further research \comment{TODO}}

\section{Reproducibility \comment{TODO}}
\section{Ethics \comment{TODO}}
\comment{\ldots\ldots}

\bibliographystyle{ieeetr}
\bibliography{references}

\newpage
\chapter{Appendix}
\section{Redundancy of mirroring in $QDQ^T$}
\label{sec:red-mirr}
Suppose we had some symmetric $\Sigma \in \mathbb{R}^{2\times 2}$; we could then use orthogonal diagonalisation to write $\Sigma = QDQ^T$ for some orthogonal Q and diagonal D.

In \ref{sec:learning-pdm}, the claim was made that requiring $Q$ to be a rotation (and not a reflection) did not decrease the expressivity of the representation, i.e. all symmetric positive definite $\Sigma$ are representable as $RDR^T$ with R being a rotation. To show this, we can suppose some reflection $Q\in \mathbb{R}^{2\times 2}$, and see that $Q$ can be written as a rotation with angle $\phi$ ($R_\phi$) of a reflection in the x-axis \cite{poole2015linear}:
\begin{align}
	Q=\begin{bmatrix}
		\cos \phi  &  \sin \phi  \\
		\sin \phi  &  -\cos \phi
	\end{bmatrix} &= \begin{bmatrix}
		\cos \phi  &  -\sin \phi  \\
		\sin \phi  &  \cos \phi
	\end{bmatrix} \begin{bmatrix}
		1  &  0  \\
		0  &  -1
	\end{bmatrix} = R_\phi \begin{bmatrix}
		1  &  0  \\
		0  &  -1
	\end{bmatrix} \label{eq:refl-rot}
\end{align}
Then, we can write out the orthogonal diagonalisation using \ref{eq:refl-rot}:
\begin{align}
	\Sigma&=QDQ^T \\
	&= \left(R_\phi \begin{bmatrix}
		1  &  0  \\
		0  &  -1
	\end{bmatrix}\right)D\left(R_\phi \begin{bmatrix}
		1  &  0  \\
		0  &  -1
	\end{bmatrix}\right)^T \\
	&= R_\phi \begin{bmatrix}
		1  &  0  \\
		0  &  -1
	\end{bmatrix} \begin{bmatrix}
		\sigma_1^2  &  0  \\
		0  &  \sigma_2^2
	\end{bmatrix} \begin{bmatrix}
		1  &  0  \\
		0  &  -1
	\end{bmatrix} R_\phi^T \\
%	&= R_\phi \begin{bmatrix}
%		\sigma_1^2  &  0  \\
%		0  &  -\sigma_2^2
%	\end{bmatrix} \begin{bmatrix}
%		1  &  0  \\
%		0  &  -1
%	\end{bmatrix} R_\phi^T \\
	&= R_\phi \begin{bmatrix}
		\sigma_1^2  &  0  \\
		0  &  \sigma_2^2
	\end{bmatrix} R_\phi^T \\
	&= R_\phi D R_\phi^T \\&&\qed\nonumber
\end{align}
\newpage
\section{Clipping kernels with Lipschitz conditions \comment{TODO}}
\label{sec:lipschitz}
\comment{Todo: proof, and better name for this section}

\newpage
\section{Alternative parameterisation for $\Sigma \in \mathbb{R}^{2\times 2}$}
\label{sec:pearson}
A different way of parameterising a $2\times 2$ covariance matrix  would be to use the Pearson correlation coefficient $\rho$ instead of the angle $\phi$. 
We can then keep the covariance matrix in its Cholesky decomposed form, using a lower triangular $L$ such that $\Sigma=LL^T$. Then, for any $\boldsymbol{\theta}\in\mathbb{R}^3$, we can find the corresponding $L$:
\begin{align}
\textrm{Let }\boldsymbol{\theta}&=\begin{bmatrix}
	\theta_1\\
	\theta_2\\
	\theta_3\\
\end{bmatrix}=\begin{bmatrix}
	\log|\sigma_1| \\
	\log|\sigma_2|\\
	\tan\rho\\
\end{bmatrix}, \textrm{ then a valid $\Sigma$ would be} \\
	\Sigma &= \begin{bmatrix}
 \sigma_1^2 & \sigma_1\sigma_2\rho \\
  \sigma_1\sigma_2\rho & \sigma_2^2 \\
\end{bmatrix}
= LL^T = \begin{bmatrix}
 l_{11} & 0 \\
  l_{21} & l_{22} \\
\end{bmatrix}\begin{bmatrix}
 l_{11} & l_{21} \\
  0 & l_{22} \\
\end{bmatrix}\\
&=\begin{bmatrix}
 l_{11}^2 & l_{11}l_{21} \\
  l_{11}l_{21} & l_{21}^2 + l_{22}^2 \\
\end{bmatrix}\hspace{0.5cm}\\
\textrm{As such,}&\textrm{ we know that:}\nonumber\\
	l_{11} &= \sqrt{\sigma_1^2} =\sigma_1=\exp(\theta_1)\\
	l_{21} &= \frac{\sigma_1\sigma_2\rho}{\sigma_1}=\sigma_2\rho=\exp(\theta_2)\tanh(\theta_3) \\
%	l_{21}^2 + l_{22}^2 &= \sigma_2^2 \textrm{, so}\\
	l_{22} &= \sqrt{\sigma_2^2 - \sigma_2\rho} = \sqrt{\exp(2\theta_2)-\exp(\theta_2)\tanh(\theta_3)}
\end{align}
which is then a valid parameterisation for the Cholesky decomposed form for a $2\times 2$ positive definite matrix\footnote{To extend this parameterisation for higher dimensions, see the  Cholesky-Banachiewicz algorithm for the Cholesky decomposition}. If we keep the covariance matrix in this triangular form, we can see that the quadratic form can be calculated in an efficient manner:

(based on the PyTorch code for the multivariate normal PDF)
\begin{align}
	\mathbf{x}^T\Sigma^{-1}\mathbf{x}
	&= \mathbf{x}^T(LL^T)^{-1}\mathbf{x}\\
	&= \mathbf{x}^T(L^T)^{-1}L^{-1}\mathbf{x}\\
	&= \mathbf{x}^T(L^{-1})^{T}L^{-1}\mathbf{x}\\
	&= ((L^{-1})\mathbf{x})^T(L^{-1}\mathbf{x})\\
	&= (L^{-1}\mathbf{x})\cdot(L^{-1}\mathbf{x})\\
\textrm{Suppose }\mathbf{b}&=L^{-1}\mathbf{x}, \textrm{ then} \nonumber\\
	L\mathbf{b}&=\mathbf{x}, \textrm{ so}\nonumber\\
	\mathbf{b}&= \textsc{solve-triangular}(L, \mathbf{x})\\
	\mathbf{x}^T\Sigma^{-1}\mathbf{x} &= \mathbf{b}\cdot\mathbf{b}
\end{align}
where $\textsc{solve-triangular}$ performs efficient backsubstitution to avoid computing the inverse. This method is significantly (>5x) faster on the CPU it was tested on while still showing modest performance improvements on the GPU it was tested on ($\sim 10\%$). However, interpretation of the Pearson correlation coefficient may be more challenging compared to interpreting the angular offset of the first primary axis, and the calculation of the quadratic forms is a negligible part of the model runtime on the GPU, so it was chosen to instead parameterise $\Sigma$ with the angle $\phi$.


\end{document}
